data = "./weights/embeddings/"
[wandb]
mode = "online" # "online", "offline", "disabled"
entity = "piercus"
project = "color-palette"
name = "wpalette-overfit-768"
tags = ["reload-ckpt", "weighted-palette", "embeddings"]

[sd]
unet = "weights/models/unet.safetensors"
text_encoder = "weights/models/CLIPTextEncoderL.safetensors"
lda = "weights/models/lda.safetensors"

[latent_diffusion]
unconditional_sampling_probability = 0.1
offset_noise = 0.2

[palette_encoder]
max_colors = 8
feedforward_dim = 128
num_layers = 2
mode = 'mlp'
embedding_dim = 768
weighted_palette = false

[training]
duration = "10000:epoch"
seed = 0
batch_size = 24
gradient_accumulation = "1:step"
# clip_grad_norm = 1.0
# clip_grad_value = 1.0
evaluation_interval = "100:epoch"
evaluation_seed = 1
# num_workers = 8
# use_color_loss = false
device="cuda:0"
dtype="bfloat16"

[monitor_gradient]
patterns = ["ip_adapter.*"]

[optimizer]
optimizer = "AdamW" # "SGD", "Adam", "AdamW", "AdamW8bit", "Lion8bit"
learning_rate = 1e-4
betas = [0.9, 0.999]
eps = 1e-8
weight_decay = 1e-2

[lr_scheduler]
type = "ConstantLR"
update_interval = "1:step"
warmup = "500:step"

[dataset]
hf_repo = "refiners/unsplash-25k-cogvlm-captions"
revision = "main"
resize_image_max_size = 512
split = "train[200:1200]"

[eval_dataset]
hf_repo = "refiners/unsplash-25k-cogvlm-captions"
revision = "main"
resize_image_max_size = 512
split = "train[200:220]"

[offload_to_cpu]
use = true
[evaluation]
batch_size = 12
num_inference_steps = 30
color_bits = 4
use_short_prompts = false
db_indexes = [0, 1, 2, 3, 4, 5, 6, 7, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
prompts = [
     "A Bustling City Street",
     "A cute cat",
     #"An oil painting",
     "A photography of a beautiful woman",
    # "A pair of shoes",
    # "A group of working people"
]
condition_scale = 1.0
